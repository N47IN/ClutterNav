Episode: 1, total numsteps: 16, episode steps: 16, reward: -111
Episode: 2, total numsteps: 44, episode steps: 28, reward: -82
Episode: 3, total numsteps: 70, episode steps: 26, reward: -46
Episode: 4, total numsteps: 83, episode steps: 13, reward: 15
Episode: 5, total numsteps: 107, episode steps: 24, reward: -70
Episode: 6, total numsteps: 124, episode steps: 17, reward: -19
Episode: 7, total numsteps: 149, episode steps: 25, reward: -63
Episode: 8, total numsteps: 158, episode steps: 9, reward: -83
Episode: 9, total numsteps: 166, episode steps: 8, reward: 8
Episode: 10, total numsteps: 171, episode steps: 5, reward: 33
Episode: 11, total numsteps: 183, episode steps: 12, reward: -174
Episode: 12, total numsteps: 195, episode steps: 12, reward: -10
Episode: 13, total numsteps: 207, episode steps: 12, reward: 0
Episode: 14, total numsteps: 222, episode steps: 15, reward: -155
Episode: 15, total numsteps: 231, episode steps: 9, reward: 21
Episode: 16, total numsteps: 254, episode steps: 23, reward: -69
Episode: 17, total numsteps: 278, episode steps: 24, reward: -44
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 0 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 1 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 2 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 3 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 4 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 5 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 6 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 7 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 8 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 9 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 10 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 11 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 12 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 13 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 14 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 15 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 17 that is less than the current step 20. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Episode: 18, total numsteps: 300, episode steps: 22, reward: -40
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 18 that is less than the current step 42. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
/home/navin/projects/M2P/Re_M2P/utils_env.py:32: RuntimeWarning: invalid value encountered in divide
  normalized_features[obj_id] = (np.array(feature_list) - min_vals) / range_vals
/home/navin/projects/M2P/Re_M2P/utils_env.py:220: RuntimeWarning: invalid value encountered in divide
  all_feature_matrix = (all_feature_matrix-mean)/std
Episode: 19, total numsteps: 329, episode steps: 29, reward: -87
> /home/navin/projects/M2P/Re_M2P/model.py(95)sample()
-> action = dist.sample()  # Sample an action index
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 19 that is less than the current step 71. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
tensor([[[ 0.0354, -1.8119,  0.0945,  ...,  1.2885,  0.9065,  0.0000],
         [ 0.5107,  1.0778, -0.1228,  ...,  2.4879,  2.2712,  0.0000],
         [ 0.0259,  1.2714,  0.4468,  ...,  0.8151,  1.3988,  0.0000],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

        [[-0.0289,  1.9588,  0.8580,  ...,  1.0732,  0.6912,  0.0000],
         [ 1.5871,  0.2071,  1.1252,  ..., -1.0230, -0.9109,  0.0000],
         [ 0.1473,  0.2808,  0.4395,  ...,  1.9533,  2.0486,  0.0000],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

        [[ 0.6198,  2.2225,  0.9400,  ..., -0.9834,  0.1711,  0.0000],
         [ 1.0849, -0.6699,  0.5706,  ...,  0.8826,  1.4711,  0.0000],
         [ 0.1608, -0.3718,  1.0717,  ...,  1.8137,  1.6283,  0.0000],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

        ...,

        [[ 1.3234, -0.1484,  0.9761,  ..., -0.1235,  0.3468,  0.0000],
         [ 1.5176,  0.3712,  0.6177,  ...,  2.0304,  1.7415,  0.0000],
         [ 0.4520,  0.7256, -0.1745,  ...,  1.3026,  1.1833,  0.0000],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

        [[ 0.8142,  1.9306,  1.9102,  ..., -0.3206,  0.2570,  0.0000],
         [-0.1267,  0.4318, -0.4080,  ..., -0.6225,  0.5150,  0.0000],
         [ 0.3672,  0.6674,  0.4899,  ...,  1.8785,  1.5137,  0.0000],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

        [[-0.5909,  1.5918,  0.9722,  ...,  0.1498,  0.1788,  0.0000],
         [ 0.2268,  0.5473,  1.0346,  ...,  1.8178,  1.9253,  0.0000],
         [ 1.2588,  1.5864,  0.4579,  ...,  0.4516,  1.2662,  0.0000],
         ...,
         [-1.3754, -0.6191,  0.7566,  ...,  1.2030,  0.6356,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])
tensor([[ 0.3996,  1.1808, -3.7465,  ...,  2.1794,  2.3173,  0.0000],
        [-1.9094,  0.5459,  0.7034,  ...,  0.6100,  0.2243,  0.0000],
        [-0.3824,  0.2801,  0.8242,  ...,  0.3200,  0.9303,  0.0000],
        ...,
        [ 2.3678,  1.3615,  0.4934,  ...,  2.0035,  1.6576,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-2.1772,  0.3509,  0.7911,  ...,  0.9279,  1.7664,  0.0000]])
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]])
tensor([[False, False, False,  ..., False, False, False],
        [False, False, False,  ..., False, False, False],
        [False, False, False,  ..., False, False, False],
        ...,
        [False, False, False,  ..., False, False, False],
        [False, False, False,  ..., False, False, False],
        [False, False, False,  ..., False, False, False]])
tensor(False)
tensor(True)
tensor([0.0483, 0.0653, 0.0364, 0.0389, 0.0494, 0.0387, 0.0650, 0.0000, 0.0402,
        0.0727, 0.0373, 0.0681, 0.0580, 0.0575, 0.0293, 0.0370, 0.0253, 0.0343,
        0.0384, 0.0336, 0.0265, 0.0258, 0.0376, 0.0000, 0.0607, 0.0547, 0.0658,
        0.0858, 0.0413, 0.0277, 0.0354, 0.0392, 0.0513, 0.0439, 0.0680, 0.1049,
        0.0219, 0.0384, 0.0274, 0.0367, 0.0199, 0.0000, 0.0760, 0.0438, 0.0435,
        0.0600, 0.0814, 0.0346, 0.0551, 0.0446, 0.0372, 0.0000, 0.0589, 0.0424,
        0.0265, 0.0000, 0.0623, 0.0273, 0.0000, 0.0357, 0.0570, 0.0371, 0.0000,
        0.0458, 0.0951, 0.0000, 0.0000, 0.0739, 0.0695, 0.0000, 0.0000, 0.0787,
        0.0526, 0.0000, 0.0265, 0.0309, 0.0546, 0.0555, 0.0291, 0.0500, 0.1250,
        0.0809, 0.0310, 0.0688, 0.0462, 0.0382, 0.0488, 0.0496, 0.0368, 0.0264,
        0.1001, 0.0000, 0.0455, 0.0000, 0.0347, 0.0519, 0.0560, 0.0529, 0.0459,
        0.0437, 0.0450, 0.0205, 0.0735, 0.0477, 0.0357, 0.0818, 0.0540, 0.0000,
        0.0643, 0.0409, 0.0000, 0.0499, 0.0599, 0.0878, 0.0818, 0.0000, 0.0536,
        0.0530, 0.0336, 0.0512, 0.0814, 0.0000, 0.0000, 0.0327, 0.0294, 0.0327,
        0.0000, 0.0374, 0.0628, 0.0718, 0.0000, 0.0000, 0.1108, 0.0503, 0.0435,
        0.0254, 0.0367, 0.0000, 0.0742, 0.0000, 0.0296, 0.1497, 0.0438, 0.0561,
        0.0377, 0.0375, 0.0379, 0.0268, 0.0294, 0.0367, 0.0000, 0.0377, 0.0344,
        0.0289, 0.0000, 0.0489, 0.0728, 0.0576, 0.0577, 0.0000, 0.0535, 0.0000,
        0.0268, 0.0000, 0.0450, 0.0480, 0.0000, 0.0401, 0.0456, 0.0720, 0.0513,
        0.0000, 0.0470, 0.0000, 0.0433, 0.0296, 0.0467, 0.0495, 0.0000, 0.0825,
        0.0382, 0.0000, 0.0410, 0.0000, 0.0474, 0.0310, 0.0207, 0.0298, 0.0422,
        0.0270, 0.0522, 0.0488, 0.0450, 0.0546, 0.0709, 0.0366, 0.0591, 0.0493,
        0.0296, 0.0710, 0.0000, 0.0743, 0.0334, 0.0346, 0.0470, 0.0000, 0.0297,
        0.0464, 0.0212, 0.0780, 0.0528, 0.0000, 0.0481,    nan, 0.0000, 0.0000,
        0.0000, 0.0890, 0.0543, 0.0498, 0.0000, 0.0000, 0.0608, 0.0000, 0.0379,
        0.0362, 0.0573, 0.0412, 0.0465, 0.0367, 0.0759, 0.0000, 0.0610, 0.0257,
        0.0000, 0.0384, 0.0404, 0.0280, 0.0375, 0.0534, 0.0451, 0.0252, 0.0334,
        0.0512, 0.0515, 0.0334, 0.1039, 0.0392, 0.0703, 0.0277, 0.0283, 0.0401,
        0.0000, 0.0298, 0.0000, 0.0316])
*** IndexError: too many indices for tensor of dimension 2
torch.Size([256, 30])
*** SyntaxError: closing parenthesis '}' does not match opening parenthesis '['
torch.Size([256])
tensor([[0.1180, 0.0635, 0.0457, 0.0458, 0.0451, 0.0420, 0.0780, 0.0981, 0.0783,
         0.0412, 0.0465, 0.0564, 0.0483, 0.0433, 0.0569, 0.0457, 0.0474, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0254, 0.0374, 0.0442, 0.0318, 0.0210, 0.0296, 0.0360, 0.0229, 0.0214,
         0.0327, 0.0298, 0.0490, 0.0354, 0.0371, 0.0231, 0.0360, 0.0398, 0.0345,
         0.0516, 0.0255, 0.0254, 0.0261, 0.0495, 0.0540, 0.0480, 0.0460, 0.0562,
         0.0310, 0.0000, 0.0000],
        [0.0433, 0.0581, 0.0481, 0.0360, 0.0501, 0.0631, 0.0704, 0.0232, 0.0552,
         0.0349, 0.0338, 0.0471, 0.0570, 0.0573, 0.0527, 0.0413, 0.0194, 0.0598,
         0.0278, 0.0284, 0.0640, 0.0291, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0462, 0.0714, 0.0483, 0.0675, 0.0829, 0.0691, 0.0398, 0.1090, 0.0747,
         0.0936, 0.0646, 0.0431, 0.1001, 0.0897, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0815, 0.0905, 0.0441, 0.0593, 0.0619, 0.0388, 0.1108, 0.0654, 0.0545,
         0.0736, 0.0794, 0.1167, 0.0814, 0.0420, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0891, 0.0634, 0.0587, 0.1377, 0.0809, 0.1107, 0.0824, 0.0839, 0.0947,
         0.0855, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0278, 0.0417, 0.0277, 0.0447, 0.0304, 0.0373, 0.0331, 0.0334, 0.0411,
         0.0233, 0.0429, 0.0323, 0.0382, 0.0303, 0.0482, 0.0444, 0.0270, 0.0658,
         0.0432, 0.0421, 0.0436, 0.0301, 0.0670, 0.0347, 0.0355, 0.0341, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0465, 0.0645, 0.0539, 0.0591, 0.0640, 0.0733, 0.0411, 0.1026, 0.0602,
         0.0580, 0.0742, 0.1208, 0.0528, 0.1290, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000],
        [0.0628, 0.0638, 0.0704, 0.0756, 0.0785, 0.0579, 0.0501, 0.1427, 0.0547,
         0.0428, 0.0524, 0.0406, 0.0451, 0.0751, 0.0875, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000]])
tensor([[0.1180, 0.0635, 0.0457,  ..., 0.0000, 0.0000, 0.0000],
        [0.0365, 0.0411, 0.0508,  ..., 0.0000, 0.0000, 0.0000],
        [0.0273, 0.0270, 0.0275,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0461, 0.0382, 0.0722,  ..., 0.0000, 0.0000, 0.0000],
        [0.0853, 0.1051, 0.1249,  ..., 0.0000, 0.0000, 0.0000],
        [0.0290, 0.0241, 0.0221,  ..., 0.0520, 0.0000, 0.0000]])
tensor([[0.1180, 0.0635, 0.0457,  ..., 0.0000, 0.0000, 0.0000],
        [0.0365, 0.0411, 0.0508,  ..., 0.0000, 0.0000, 0.0000],
        [0.0273, 0.0270, 0.0275,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0461, 0.0382, 0.0722,  ..., 0.0000, 0.0000, 0.0000],
        [0.0853, 0.1051, 0.1249,  ..., 0.0000, 0.0000, 0.0000],
        [0.0290, 0.0241, 0.0221,  ..., 0.0520, 0.0000, 0.0000]])
tensor([[False, False, False,  ..., False, False, False],
        [False, False, False,  ..., False, False, False],
        [False, False, False,  ..., False, False, False],
        ...,
        [False, False, False,  ..., False, False, False],
        [False, False, False,  ..., False, False, False],
        [False, False, False,  ..., False, False, False]])
tensor(True)
--KeyboardInterrupt--
