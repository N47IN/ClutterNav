Episode: 1, total numsteps: 15, episode steps: 15, reward: -34
Episode: 2, total numsteps: 30, episode steps: 15, reward: -53
Episode: 3, total numsteps: 45, episode steps: 15, reward: -95
Episode: 4, total numsteps: 60, episode steps: 15, reward: -47
Episode: 5, total numsteps: 75, episode steps: 15, reward: -17
Episode: 6, total numsteps: 90, episode steps: 15, reward: -29
Episode: 7, total numsteps: 105, episode steps: 15, reward: -49
Episode: 8, total numsteps: 120, episode steps: 15, reward: -46
Episode: 9, total numsteps: 135, episode steps: 15, reward: -37
Episode: 10, total numsteps: 150, episode steps: 15, reward: -55
Episode: 11, total numsteps: 165, episode steps: 15, reward: -20
Episode: 12, total numsteps: 180, episode steps: 15, reward: -20
Episode: 13, total numsteps: 195, episode steps: 15, reward: -48
Episode: 14, total numsteps: 210, episode steps: 15, reward: -26
Episode: 15, total numsteps: 225, episode steps: 15, reward: -20
Episode: 16, total numsteps: 240, episode steps: 15, reward: -52
Episode: 17, total numsteps: 255, episode steps: 15, reward: -56
Traceback (most recent call last):
  File "/home/navin/projects/M2P/Re_M2P/agent5_sac.py", line 94, in <module>
    critic_1_loss, critic_2_loss, policy_loss, ent_loss, alpha = agent.update_parameters(memory, args.batch_size, updates)
  File "/home/navin/projects/M2P/Re_M2P/sac.py", line 63, in update_parameters
    next_state_action, next_state_log_pi = self.policy.sample(next_state_batch)
  File "/home/navin/projects/M2P/Re_M2P/model.py", line 90, in sample
    probs = self.forward(state)  # Get probabilities
  File "/home/navin/projects/M2P/Re_M2P/model.py", line 81, in forward
    x = F.relu(self.linear1(state))
  File "/home/navin/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/navin/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/navin/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (7680x13 and 12x256)
/usr/lib/python3/dist-packages/apport/report.py:13: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import fnmatch, glob, traceback, errno, sys, atexit, imp, stat
Traceback (most recent call last):
  File "/home/navin/projects/M2P/Re_M2P/agent5_sac.py", line 94, in <module>
    critic_1_loss, critic_2_loss, policy_loss, ent_loss, alpha = agent.update_parameters(memory, args.batch_size, updates)
  File "/home/navin/projects/M2P/Re_M2P/sac.py", line 63, in update_parameters
    next_state_action, next_state_log_pi = self.policy.sample(next_state_batch)
  File "/home/navin/projects/M2P/Re_M2P/model.py", line 90, in sample
    probs = self.forward(state)  # Get probabilities
  File "/home/navin/projects/M2P/Re_M2P/model.py", line 81, in forward
    x = F.relu(self.linear1(state))
  File "/home/navin/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/navin/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/navin/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (7680x13 and 12x256)
