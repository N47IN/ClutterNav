Episode: 1, total numsteps: 16, episode steps: 16, reward: -101
Episode: 2, total numsteps: 44, episode steps: 28, reward: -82
Episode: 3, total numsteps: 70, episode steps: 26, reward: -46
Episode: 4, total numsteps: 83, episode steps: 13, reward: 15
Episode: 5, total numsteps: 107, episode steps: 24, reward: -70
Episode: 6, total numsteps: 124, episode steps: 17, reward: -19
Episode: 7, total numsteps: 149, episode steps: 25, reward: -63
Episode: 8, total numsteps: 158, episode steps: 9, reward: -73
Episode: 9, total numsteps: 166, episode steps: 8, reward: 8
Episode: 10, total numsteps: 171, episode steps: 5, reward: 33
Episode: 11, total numsteps: 183, episode steps: 12, reward: -164
Episode: 12, total numsteps: 195, episode steps: 12, reward: -10
Episode: 13, total numsteps: 207, episode steps: 12, reward: 0
Episode: 14, total numsteps: 222, episode steps: 15, reward: -145
Episode: 15, total numsteps: 231, episode steps: 9, reward: 21
Episode: 16, total numsteps: 254, episode steps: 23, reward: -69
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 0 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 1 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 2 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 3 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 4 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 5 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 6 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 7 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Episode: 17, total numsteps: 278, episode steps: 24, reward: -44
Episode: 18, total numsteps: 300, episode steps: 22, reward: -40
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 8 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 9 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 10 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 11 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 12 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 13 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 14 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 15 that is less than the current step 16. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 17 that is less than the current step 20. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 18 that is less than the current step 42. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
/home/navin/projects/M2P/Re_M2P/utils_env.py:32: RuntimeWarning: invalid value encountered in divide
  normalized_features[obj_id] = (np.array(feature_list) - min_vals) / range_vals
/home/navin/projects/M2P/Re_M2P/utils_env.py:223: RuntimeWarning: invalid value encountered in divide
  all_feature_matrix = (all_feature_matrix-mean)/std
Episode: 19, total numsteps: 329, episode steps: 29, reward: -87
> /home/navin/projects/M2P/Re_M2P/model.py(95)sample()
-> action = dist.sample()  # Sample an action index
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 19 that is less than the current step 71. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
tensor([[[ 0.0354, -1.8119,  0.0945,  ...,  1.2885,  0.9065,  0.0000],
         [ 0.5107,  1.0778, -0.1228,  ...,  2.4879,  2.2712,  0.0000],
         [ 0.0259,  1.2714,  0.4468,  ...,  0.8151,  1.3988,  0.0000],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

        [[-0.0289,  1.9588,  0.8580,  ...,  1.0732,  0.6912,  0.0000],
         [ 1.5871,  0.2071,  1.1252,  ..., -1.0230, -0.9109,  0.0000],
         [ 0.1473,  0.2808,  0.4395,  ...,  1.9533,  2.0486,  0.0000],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

        [[ 0.6198,  2.2225,  0.9400,  ..., -0.9834,  0.1711,  0.0000],
         [ 1.0849, -0.6699,  0.5706,  ...,  0.8826,  1.4711,  0.0000],
         [ 0.1608, -0.3718,  1.0717,  ...,  1.8137,  1.6283,  0.0000],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

        ...,

        [[ 1.3234, -0.1484,  0.9761,  ..., -0.1235,  0.3468,  0.0000],
         [ 1.5176,  0.3712,  0.6177,  ...,  2.0304,  1.7415,  0.0000],
         [ 0.4520,  0.7256, -0.1745,  ...,  1.3026,  1.1833,  0.0000],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

        [[ 0.8142,  1.9306,  1.9102,  ..., -0.3206,  0.2570,  0.0000],
         [-0.1267,  0.4318, -0.4080,  ..., -0.6225,  0.5150,  0.0000],
         [ 0.3672,  0.6674,  0.4899,  ...,  1.8785,  1.5137,  0.0000],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

        [[-0.5909,  1.5918,  0.9722,  ...,  0.1498,  0.1788,  0.0000],
         [ 0.2268,  0.5473,  1.0346,  ...,  1.8178,  1.9253,  0.0000],
         [ 1.2588,  1.5864,  0.4579,  ...,  0.4516,  1.2662,  0.0000],
         ...,
         [-1.3754, -0.6191,  0.7566,  ...,  1.2030,  0.6356,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])
tensor(True)
*** TypeError: where() received an invalid combination of arguments - got (), but expected one of:
 * (Tensor condition, Tensor other)
 * (Tensor condition, Number other)
<built-in method where of Tensor object at 0x7f789c5cc130>
tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]])
*** AttributeError: 'Tensor' object has no attribute 'shpae'
torch.Size([256, 30, 13])
tensor([[213,   0,   0],
        [213,   0,   1],
        [213,   0,   2],
        [213,   0,   3],
        [213,   0,   4],
        [213,   0,   5],
        [213,   0,   6],
        [213,   0,   7],
        [213,   0,   8],
        [213,   0,   9],
        [213,   0,  10],
        [213,   0,  11]])
Traceback (most recent call last):
  File "agent5_sac.py", line 96, in <module>
    critic_1_loss, critic_2_loss, policy_loss, ent_loss, alpha = agent.update_parameters(memory, args.batch_size, updates)
  File "/home/navin/projects/M2P/Re_M2P/sac.py", line 63, in update_parameters
    next_state_action, next_state_log_pi = self.policy.sample(next_state_batch)
  File "/home/navin/projects/M2P/Re_M2P/model.py", line 95, in sample
    action = dist.sample()  # Sample an action index
UnboundLocalError: local variable 'dist' referenced before assignment
/usr/lib/python3/dist-packages/apport/report.py:13: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import fnmatch, glob, traceback, errno, sys, atexit, imp, stat
Traceback (most recent call last):
  File "agent5_sac.py", line 96, in <module>
    critic_1_loss, critic_2_loss, policy_loss, ent_loss, alpha = agent.update_parameters(memory, args.batch_size, updates)
  File "/home/navin/projects/M2P/Re_M2P/sac.py", line 63, in update_parameters
    next_state_action, next_state_log_pi = self.policy.sample(next_state_batch)
  File "/home/navin/projects/M2P/Re_M2P/model.py", line 95, in sample
    action = dist.sample()  # Sample an action index
UnboundLocalError: local variable 'dist' referenced before assignment
